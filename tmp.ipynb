{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,load_dataset\n",
    "import json\n",
    "datasets = []\n",
    "with open(\n",
    "    \"/data3/yss/sft_datas/0912/sft_data_merge_v18_quality_filtered_sharegpt.jsonl\",\n",
    "    mode=\"r\",\n",
    ") as f:\n",
    "    for line in f:\n",
    "        datasets.append(json.loads(line))\n",
    "print(len(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10000 examples [00:00, 12567.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tools', 'system', 'conversations'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datas = []\n",
    "for data in datasets[0:10000]:\n",
    "    data_new = {}\n",
    "    if \"tools\" in data:\n",
    "        if isinstance(data[\"tools\"],list):\n",
    "            if len(data[\"tools\"]):\n",
    "                data[\"tools\"] = json.dumps(data[\"tools\"])\n",
    "            else:\n",
    "                data[\"tools\"] = \"\"\n",
    "        elif data[\"tools\"] ==\"[]\" or data[\"tools\"] ==\"[ ]\" or data[\"tools\"] ==\" \":\n",
    "            data[\"tools\"] = \"\"\n",
    "        else:\n",
    "            data[\"tools\"] = data[\"tools\"]\n",
    "    else:\n",
    "        data[\"tools\"] = \"\"\n",
    "\n",
    "    data_new[\"tools\"] = data[\"tools\"]\n",
    "    data_new[\"system\"] = data[\"system\"]\n",
    "    data_new[\"conversations\"] = data[\"conversations\"]\n",
    "    datas.append(data_new)\n",
    "    \n",
    "# data_path_trans = \"/data3/yss/sft_datas/0912/sft_data_merge_v18_quality_filtered_sharegpt_trans.json\"\n",
    "data_path_trans = \"./sft_data_merge_v18_quality_filtered_sharegpt_trans.json\"\n",
    "# save_jsonl(datas, data_path_trans)\n",
    "with open(data_path_trans,\"w\")as f:\n",
    "    json.dump(datas,f,ensure_ascii=False)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=data_path_trans,\n",
    "    split=\"train\",\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>human\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"/data0/pretrained-models/Qwen2-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "print(model)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dschat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
